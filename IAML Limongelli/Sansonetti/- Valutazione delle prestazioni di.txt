- Valutazione delle prestazioni di un sistema di Classificazione
Esistono vari metodi e metriche per poter valutare le prestazioni di un Classificatore, la prima metrica, che è la più intuitiva, è l'accuratezza che rispecchia direttamente la funzione obiettivo del classificatore. Essa misura quanti pattern sono stati correttamente classificati rispetto al numero di pattern classificati totale.
Accuratezza = patterncorrettamenteclassificati/patternclassificati
Non sempre l'accuratezza è la metrica adeguata a descrivere e valutare le prestazioni di un classificatore, per esempio nel caso di un classificatore binario potrebbe non rappresentare correttamente l'accuratezza. Per questi casi estremi vengono utilizzate metriche che sono più legate alla semantica del problema come Precision e Recall.
Nella Regressione viene tipicamente utilizzato il valore chiamato RMSE, (Root Mean Squared Error) RMSE= rad(1/N SUM(pred-true)^2), ovvero la radice della differenza tra il valore predetto e il valore vero al quadrato.
La matrice di confusione è molto utile nei problemi di classificazione per capire meglio dove sono presenti e dove si concentrano gli errori. Le righe della matrice rappresentano le classi vere dei pattern che abbiamo classificato, mentre le colonne della matrice rappresentano le classi predette dal classificatore per ciascun pattern. In ogni cella c'è quindi la percentuale di "successo" del nostro classificatore nel predire la classe di quel pattern. Più la nostra matrice è diagonale e quindi avrà valori alti sulla diagonale più il nostro classificatore sarà accurato, valori elevati al di fuori della diagonale indica una concentrazione alta di errori in quelle celle.
- Precision e Recall
Tra le metriche che possono essere utilizzate per la valutazione delle prestazioni di un classificatore, c'è precision e recall, adatte anche a problemi con un numero di classi non comune come la classificazione binaria, dove l'accuratezza come metrica potrebbe non rappresentare correttamente le prestazioni del classificatore.
Precision: La precision misura quanto il classificatore sia preciso e accurato nella sua funzione, i valori della precision vanno da 0 a 1 e se vengono ottenuti valori bassi il classificatore è da ritenere poco preciso viceversa se i valori sono alti. La formula è Precision = patterncorrettamenteclassificati/patternclassificati = TP/(TP+FP) dove TP = True Positives e FP = False Positives
Recall: La metrica Recall indica quanto un classificatore sia selettivo nella classificazione dei pattern, in sostanza se un classificatore tende a scegliere con un alto rigore le classi per i pattern, avendo molti falsi negativi a causa della sua selezione rigorosa, avrà un valore di Recall molto alto, che va sempre da 0 ad 1. Dunque l'obiettivo è tenere il valore di recall il più basso possibile. La sua formula è:
Recall = TP/(TP+FN) = TP/P = TPR (True Positives Rate)
Per rappresentare meglio queste due metriche è molto utile la matrice di confusione 2x2 che evidenzia i TP,FP,TN,FN.
In ambito medico vengono usate delle metriche simili di nome Specificity e Sensibility, che seguono le stesse funzioni di Precision e Recall rispettivamente
- Iperparametri:
Gli iperparametri sono dei parametri che vengono scelti nella fase precedente all'apprendimento, sono richiesti in molti algoritmi e alcuni utilizzi per gli iperparametri sono questi: il numero di neuroni in una rete neurale, il numero di vicini K in un classificatore K-NN, la tipologia di loss function, il grado del polinomio di una regressione. Il loro utilizzo funziona a 2 strati, i valori iniziali vengono scelti casualmente, di seguito viene effettuato l'apprendimento e in base all'accuratezza e ai valori in uscita vengono adattati i valori degli iperparametri per rieffettuare l'apprendimento. Per ottimizzare la ricerca degli iperparametri migliori esistono vari algoritmi tra cui: K-fold cross validation, grid search e random search
K-fold cross validation parte scegliendo un numero di "fold" che sarà il nostro K, il dataset iniziale viene diviso in training set e test set, poi il training set viene diviso in tante sezioni quante sono i fold scelti prima in k. Successivamente viene effettuato il training k volte scegliendo ogni volta un fold diverso da utilizzare come valid set. Viene dunque calcolata l'avg_acc e verranno scelti gli iperparametri con l'avg_acc più alta tra tutti. Il caso più estremo avviene quando come k viene scelto il totale dei pattern, dunque il training sarà ripetuto per ciascun pattern e ciascun valid
Il grid-search riceve degli intervalli di valori dei parametri su cui fare il training iterativo, può in casi estremi costare molte risorse dato che prova tutte le combinazioni dei valori dei parametri.
Il random-search invece prende dei valori casuali all'interno degli intervalli degli iperparametri, ne calcola l'accuratezza e si ferma dopo una quantità di iterazioni decisa prima della sua esecuzione. 